{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T04:11:34.820846Z",
     "start_time": "2024-12-19T04:10:18.963637Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet import preprocess_input"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T18:38:52.031603Z",
     "start_time": "2024-12-18T16:17:44.829416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# **Step 1: Define data generator**\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X_path, y_path, indices, batch_size):\n",
    "        self.X = np.memmap(X_path, dtype='float32', mode='r', shape=(726, 15, 224, 224, 3))\n",
    "        self.y = np.memmap(y_path, dtype='int32', mode='r', shape=(726,))\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X_batch = self.X[batch_indices]\n",
    "        X_batch = preprocess_input(X_batch)  # Preprocess for ResNet\n",
    "        y_batch = np.eye(2)[self.y[batch_indices]]  # One-hot encode\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "class StepTimerCallback(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\n--- Starting Epoch {epoch + 1} ---\")\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        print(f\"--- Epoch {epoch + 1} completed in {epoch_time:.2f} seconds ---\\n\")\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        self.step_start_time = time.time()\n",
    "        print(f\"Step {batch + 1}/{self.params['steps']} - \", end=\"\")\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        step_time = time.time() - self.step_start_time\n",
    "        print(f\"Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, Time: {step_time:.2f} seconds\")\n",
    "        \n",
    "class BatchEarlyStopping(Callback):\n",
    "    def __init__(self, monitor='loss', threshold=0.1, patience=5):\n",
    "        \"\"\"\n",
    "        Early stopping within the same epoch based on a monitored metric.\n",
    "        Args:\n",
    "            monitor: Metric to monitor ('loss', 'accuracy', etc.).\n",
    "            threshold: Threshold value for stopping (e.g., loss < 0.1).\n",
    "            patience: Number of batches to wait for improvement before stopping.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        current_value = logs.get(self.monitor)\n",
    "        if current_value is not None:\n",
    "            # Check if the monitored metric meets the threshold\n",
    "            if current_value < self.threshold:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    print(f\"\\nEarly stopping triggered at batch {batch + 1}: {self.monitor} = {current_value:.4f}\")\n",
    "                    self.model.stop_training = True\n",
    "            else:\n",
    "                self.wait = 0  # Reset patience if condition is not met\n",
    "\n",
    "# Instantiate the batch-level early stopping callback\n",
    "batch_early_stopping_callback = BatchEarlyStopping(\n",
    "    monitor='loss',      # Metric to monitor\n",
    "    threshold=0.1,       # Stop if loss goes below this value\n",
    "    patience=2           # Number of consecutive batches meeting the condition\n",
    ")\n",
    "\n",
    "# **Step 2: Build the CNN-LSTM model**\n",
    "def build_cnn_lstm_model(seq_length, height, width, channels, num_classes):\n",
    "    # Use MobileNet as the base model\n",
    "    base_model = ResNet(weights='imagenet', include_top=False, input_shape=(height, width, channels))\n",
    "    base_model.trainable = False  # Freeze the pre-trained model\n",
    "\n",
    "    # Input for the sequence of frames\n",
    "    sequence_input = Input(shape=(seq_length, height, width, channels))\n",
    "    \n",
    "    # TimeDistributed wrapper to apply the CNN to each frame\n",
    "    cnn_features = TimeDistributed(base_model)(sequence_input)\n",
    "    flattened_features = TimeDistributed(Flatten())(cnn_features)\n",
    "\n",
    "    # Single LSTM layer for temporal modeling\n",
    "    lstm_out = LSTM(128)(flattened_features)\n",
    "    lstm_out = Dropout(0.5)(lstm_out)\n",
    "\n",
    "    # Single fully connected layer and output\n",
    "    dense_out = Dense(64, activation='relu')(lstm_out)\n",
    "    dense_out = Dropout(0.5)(dense_out)\n",
    "    output = Dense(num_classes, activation='softmax')(dense_out)\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(inputs=sequence_input, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "cnn_lstm_model = build_cnn_lstm_model(seq_length=15, height=224, width=224, channels=3, num_classes=2)\n",
    "cnn_lstm_model.summary()\n",
    "\n",
    "# **Step 3: Split dataset into train and validation**\n",
    "indices = np.arange(726)\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_combined.dat', r'E:\\PosePerfect\\Dataset Creation\\y_combined.dat', train_indices, batch_size=11)\n",
    "val_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_combined.dat', r'E:\\PosePerfect\\Dataset Creation\\y_combined.dat', val_indices, batch_size=11)\n",
    "\n",
    "# **Step 4: Set up checkpoint callback**\n",
    "checkpoint_dir = './Checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}_val_loss_{val_loss:.2f}.keras')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    save_best_only=False,  # Save at every epoch regardless of performance\n",
    "    save_weights_only=False,  # Save the entire model (not just weights)\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# **Step 6: Learning Rate Scheduler**\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 2:  # Keep the initial learning rate for the first 2 epochs\n",
    "        return lr\n",
    "    return lr * 0.9  # Decay the learning rate by 10% after every epoch\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "step_timer_callback = StepTimerCallback()\n",
    "\n",
    "# Step 7: Train the model \n",
    "cnn_lstm_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_gen),\n",
    "    validation_steps=len(val_gen),\n",
    "    callbacks=[checkpoint_callback, lr_callback, step_timer_callback, batch_early_stopping_callback],\n",
    "    verbose=1  # Ensure verbose is enabled\n",
    ")\n"
   ],
   "id": "f5db45c1db0e7197",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "82cb1cd8964e7a8e",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
