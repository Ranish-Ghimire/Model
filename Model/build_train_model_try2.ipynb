{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T14:17:21.344035Z",
     "start_time": "2024-12-16T14:16:30.201479Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T12:33:28.065873Z",
     "start_time": "2024-12-14T12:33:28.040558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# **Step 1: Define data generator**\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X_path, y_path, indices, batch_size):\n",
    "        self.X = np.memmap(X_path, dtype='float32', mode='r', shape=(5082, 10, 224, 224, 3))\n",
    "        self.y = np.memmap(y_path, dtype='int32', mode='r', shape=(5082,))\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = np.eye(2)[self.y[batch_indices]]  # One-hot encode\n",
    "        return X_batch, y_batch"
   ],
   "id": "f1ca2beb05614a04",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-14T12:33:54.987589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# **Step 1: Define data generator**\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X_path, y_path, indices, batch_size):\n",
    "        self.X = np.memmap(X_path, dtype='float32', mode='r', shape=(5082, 10, 224, 224, 3))\n",
    "        self.y = np.memmap(y_path, dtype='int32', mode='r', shape=(5082,))\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = np.eye(2)[self.y[batch_indices]]  # One-hot encode\n",
    "        return X_batch, y_batch\n",
    "\n",
    "# **Step 2: Build the CNN-LSTM model**\n",
    "def build_cnn_lstm_model(seq_length, height, width, channels, num_classes):\n",
    "    model = Sequential([\n",
    "        TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                               kernel_regularizer=l2(0.01)),  # Regularization added\n",
    "                        input_shape=(seq_length, height, width, channels)),\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D(pool_size=(2, 2))),\n",
    "\n",
    "        TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                               kernel_regularizer=l2(0.01))),  # Regularization added\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D(pool_size=(2, 2))),\n",
    "\n",
    "        TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                               kernel_regularizer=l2(0.01))),  # Regularization added\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D(pool_size=(2, 2))),\n",
    "\n",
    "        TimeDistributed(Flatten()),\n",
    "\n",
    "        LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)),  # Regularization added\n",
    "        Dropout(0.5),\n",
    "        LSTM(64, kernel_regularizer=l2(0.01)),  # Regularization added\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),  # Regularization added\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))  # Regularization added\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)  # Lower initial learning rate\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "cnn_lstm_model = build_cnn_lstm_model(seq_length=10, height=224, width=224, channels=3, num_classes=2)\n",
    "cnn_lstm_model.summary()\n",
    "\n",
    "# **Step 3: Split dataset into train and validation**\n",
    "indices = np.arange(5082)\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_final.dat', r'E:\\PosePerfect\\Dataset Creation\\y_final.dat', train_indices, batch_size=8)\n",
    "val_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_final.dat', r'E:\\PosePerfect\\Dataset Creation\\y_final.dat', val_indices, batch_size=8)\n",
    "\n",
    "# **Step 4: Set up checkpoint callback**\n",
    "checkpoint_dir = './Checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}_val_loss_{val_loss:.2f}.keras')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    save_best_only=False,  # Save at every epoch regardless of performance\n",
    "    save_weights_only=False,  # Save the entire model (not just weights)\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# **Step 5: Set up real-time accuracy and loss callback**\n",
    "def on_batch_end(batch, logs):\n",
    "    # Print loss and accuracy after each batch during the epoch\n",
    "    print(f\"Epoch: {logs['epoch']+1}, Batch: {batch + 1}/{logs['steps']}, Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}\")\n",
    "\n",
    "realtime_metrics_callback = LambdaCallback(on_batch_end=on_batch_end)\n",
    "\n",
    "\n",
    "# **Step 6: Learning Rate Scheduler**\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 2:  # Keep the initial learning rate for the first 2 epochs\n",
    "        return lr\n",
    "    return lr * 0.9  # Decay the learning rate by 10% after every epoch\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Step 7: Train the model \n",
    "cnn_lstm_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_gen),\n",
    "    validation_steps=len(val_gen),\n",
    "    callbacks=[checkpoint_callback, realtime_metrics_callback],\n",
    "    verbose=1# Add checkpointing and real-time metrics callback\n",
    ")\n"
   ],
   "id": "de00e7dc91fbf2e5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_final.dat', r'E:\\PosePerfect\\Dataset Creation\\y_final.dat', train_indices, batch_size=10)\n",
    "val_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_final.dat', r'E:\\PosePerfect\\Dataset Creation\\y_final.dat', val_indices, batch_size=10)"
   ],
   "id": "805151120bd029de",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:43:39.577661Z",
     "start_time": "2024-12-15T12:43:39.562238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StepTimerCallback(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\n--- Starting Epoch {epoch + 1} ---\")\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        print(f\"--- Epoch {epoch + 1} completed in {epoch_time:.2f} seconds ---\\n\")\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        self.step_start_time = time.time()\n",
    "        print(f\"Step {batch + 1}/{self.params['steps']} - \", end=\"\")\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        step_time = time.time() - self.step_start_time\n",
    "        print(f\"Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, Time: {step_time:.2f} seconds\")"
   ],
   "id": "b7604ee916bf7ebf",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-15T12:43:44.166207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# **Step 1: Define data generator**\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X_path, y_path, indices, batch_size):\n",
    "        self.X = np.memmap(X_path, dtype='float32', mode='r', shape=(2041, 10, 224, 224, 3))\n",
    "        self.y = np.memmap(y_path, dtype='int32', mode='r', shape=(2041,))\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = np.eye(2)[self.y[batch_indices]]  # One-hot encode\n",
    "        return X_batch, y_batch\n",
    "\n",
    "# **Step 2: Build the CNN-LSTM model**\n",
    "def build_cnn_lstm_model(seq_length, height, width, channels, num_classes):\n",
    "    model = Sequential([\n",
    "        TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                               kernel_regularizer=l2(0.01)),  # Regularization added\n",
    "                        input_shape=(seq_length, height, width, channels)),\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D(pool_size=(2, 2))),\n",
    "\n",
    "        TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                               kernel_regularizer=l2(0.01))),  # Regularization added\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D(pool_size=(2, 2))),\n",
    "\n",
    "        TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                               kernel_regularizer=l2(0.01))),  # Regularization added\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D(pool_size=(2, 2))),\n",
    "\n",
    "        TimeDistributed(Flatten()),\n",
    "\n",
    "        LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)),  # Regularization added\n",
    "        Dropout(0.5),\n",
    "        LSTM(64, kernel_regularizer=l2(0.01)),  # Regularization added\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),  # Regularization added\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))  # Regularization added\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)  # Lower initial learning rate\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "cnn_lstm_model = build_cnn_lstm_model(seq_length=10, height=224, width=224, channels=3, num_classes=2)\n",
    "cnn_lstm_model.summary()\n",
    "\n",
    "# **Step 3: Split dataset into train and validation**\n",
    "indices = np.arange(2041)\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_final_org.dat', r'E:\\PosePerfect\\Dataset Creation\\y_final_org.dat', train_indices, batch_size=10)\n",
    "val_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_final_org.dat', r'E:\\PosePerfect\\Dataset Creation\\y_final_org.dat', val_indices, batch_size=10)\n",
    "\n",
    "# **Step 4: Set up checkpoint callback**\n",
    "checkpoint_dir = './Checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}_val_loss_{val_loss:.2f}.keras')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    save_best_only=False,  # Save at every epoch regardless of performance\n",
    "    save_weights_only=False,  # Save the entire model (not just weights)\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# **Step 6: Learning Rate Scheduler**\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 2:  # Keep the initial learning rate for the first 2 epochs\n",
    "        return lr\n",
    "    return lr * 0.9  # Decay the learning rate by 10% after every epoch\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "step_timer_callback = StepTimerCallback()\n",
    "\n",
    "# Step 7: Train the model \n",
    "cnn_lstm_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_gen),\n",
    "    validation_steps=len(val_gen),\n",
    "    callbacks=[checkpoint_callback, lr_callback, step_timer_callback],\n",
    "    verbose=1  # Ensure verbose is enabled\n",
    ")\n"
   ],
   "id": "2a540a8aa951983c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-16T14:03:37.629837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# **Step 1: Data Generator**\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X_path, y_path, indices, batch_size):\n",
    "        self.X = np.memmap(X_path, dtype='float32', mode='r', shape=(2041, 10, 224, 224, 3))\n",
    "        self.y = np.memmap(y_path, dtype='int32', mode='r', shape=(2041,))\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = np.eye(2)[self.y[batch_indices]]  # One-hot encode\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "class StepTimerCallback(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\n--- Starting Epoch {epoch + 1} ---\")\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        print(f\"--- Epoch {epoch + 1} completed in {epoch_time:.2f} seconds ---\\n\")\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        self.step_start_time = time.time()\n",
    "        print(f\"Step {batch + 1}/{self.params['steps']} - \", end=\"\")\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        step_time = time.time() - self.step_start_time\n",
    "        print(f\"Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, Time: {step_time:.2f} seconds\")\n",
    "        \n",
    "class BatchEarlyStopping(Callback):\n",
    "    def __init__(self, monitor='loss', threshold=0.1, patience=5):\n",
    "        \"\"\"\n",
    "        Early stopping within the same epoch based on a monitored metric.\n",
    "        Args:\n",
    "            monitor: Metric to monitor ('loss', 'accuracy', etc.).\n",
    "            threshold: Threshold value for stopping (e.g., loss < 0.1).\n",
    "            patience: Number of batches to wait for improvement before stopping.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        current_value = logs.get(self.monitor)\n",
    "        if current_value is not None:\n",
    "            # Check if the monitored metric meets the threshold\n",
    "            if current_value < self.threshold:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    print(f\"\\nEarly stopping triggered at batch {batch + 1}: {self.monitor} = {current_value:.4f}\")\n",
    "                    self.model.stop_training = True\n",
    "            else:\n",
    "                self.wait = 0  # Reset patience if condition is not met\n",
    "\n",
    "# Instantiate the batch-level early stopping callback\n",
    "batch_early_stopping_callback = BatchEarlyStopping(\n",
    "    monitor='loss',      # Metric to monitor\n",
    "    threshold=0.1,       # Stop if loss goes below this value\n",
    "    patience=2           # Number of consecutive batches meeting the condition\n",
    ")\n",
    "\n",
    "# **Step 2: Learning Rate Scheduler**\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 2:  # Keep the initial learning rate for the first 2 epochs\n",
    "        return lr\n",
    "    return lr * 0.9  # Decay the learning rate by 10% after every epoch\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# **Step 3: Real-time Accuracy and Loss Callback**\n",
    "def on_batch_end(batch, logs):\n",
    "    print(f\"Epoch: {logs.get('epoch', 0) + 1}, Batch: {batch + 1}, Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}\")\n",
    "\n",
    "realtime_metrics_callback = LambdaCallback(on_batch_end=on_batch_end)\n",
    "\n",
    "# **Step 4: Split Dataset**\n",
    "indices = np.arange(2041)\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_final_org.dat', r'E:\\PosePerfect\\Dataset Creation\\y_final_org.dat', train_indices, batch_size=10)\n",
    "val_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_final_org.dat', r'E:\\PosePerfect\\Dataset Creation\\y_final_org.dat', val_indices, batch_size=10)\n",
    "\n",
    "# **Step 5: Load Checkpoint and Resume Training**\n",
    "# Path to the saved model\n",
    "checkpoint_path = r'E:\\PosePerfect\\Model\\Checkpoints\\model_epoch_01_val_loss_6.53.keras'\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "cnn_lstm_model = load_model(checkpoint_path)\n",
    "\n",
    "# Verify model structure\n",
    "cnn_lstm_model.summary()\n",
    "\n",
    "# Set up a new checkpoint callback to save subsequent epochs\n",
    "new_checkpoint_dir = './Checkpoints'\n",
    "os.makedirs(new_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "new_checkpoint_path = os.path.join(new_checkpoint_dir, 'model_epoch_{epoch:02d}_val_loss_{val_loss:.2f}.keras')\n",
    "new_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=new_checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "step_timer_callback = StepTimerCallback()\n",
    "\n",
    "\n",
    "# **Step 6: Resume Training**\n",
    "cnn_lstm_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,  # Total number of epochs (not just resuming epochs)\n",
    "    initial_epoch=1,  # Resume from epoch 1\n",
    "    steps_per_epoch=len(train_gen),\n",
    "    validation_steps=len(val_gen),\n",
    "    callbacks=[new_checkpoint_callback, step_timer_callback, lr_callback, batch_early_stopping_callback],\n",
    "    verbose=1\n",
    ")\n"
   ],
   "id": "626cdf611a7ef44c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "\n",
   "id": "c9ee20b8e4d6a979",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:17:43.337873Z",
     "start_time": "2024-12-16T14:17:30.723184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Path to your saved model from epoch 1\n",
    "checkpoint_path = r'E:\\PosePerfect\\Model\\Checkpoints\\model_epoch_01_val_loss_6.53.keras'\n",
    "\n",
    "# Load the model\n",
    "cnn_lstm_model = load_model(checkpoint_path)\n",
    "\n",
    "# Verify the loaded model structure and weights\n",
    "cnn_lstm_model.summary()\n"
   ],
   "id": "bd3375c8275e3049",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:19:24.623593Z",
     "start_time": "2024-12-16T14:19:24.519286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# **Step 1: Define data generator**\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X_path, y_path, indices, batch_size):\n",
    "        self.X = np.memmap(X_path, dtype='float32', mode='r', shape=(2041, 10, 224, 224, 3))\n",
    "        self.y = np.memmap(y_path, dtype='int32', mode='r', shape=(2041,))\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = np.eye(2)[self.y[batch_indices]]  # One-hot encode\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "indices = np.arange(2041)\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_final_org.dat', r'E:\\PosePerfect\\Dataset Creation\\y_final_org.dat', train_indices, batch_size=10)\n",
    "val_gen = DataGenerator(r'E:\\PosePerfect\\Dataset Creation\\X_final_org.dat', r'E:\\PosePerfect\\Dataset Creation\\y_final_org.dat', val_indices, batch_size=10)"
   ],
   "id": "efa796c705565bcb",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:20:48.160804Z",
     "start_time": "2024-12-16T14:19:27.272078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_loss, val_accuracy = cnn_lstm_model.evaluate(val_gen)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")"
   ],
   "id": "860b9c70fc97912",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:43:09.268310Z",
     "start_time": "2024-12-16T14:42:33.448166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_input = X_data = np.memmap(r'E:\\PosePerfect\\Model\\Process Example\\X_exam.dat', dtype='float32', mode='r', shape=(40, 10, 224, 224, 3))\n",
    "\n",
    "predictions = cnn_lstm_model.predict(example_input)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ],
   "id": "59496285cf47a123",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "f0a9a7afdefa93af",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
